{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34baa3e",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb28a624",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "def run_and_save(messages, model = \"gpt-3.5-turbo\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0,\n",
    "        max_tokens = 500\n",
    "    )\n",
    "    text = response.choices[0].message.content.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ec6b1",
   "metadata": {
    "height": 30
   },
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d222af6",
   "metadata": {
    "height": 181
   },
   "outputs": [],
   "source": [
    "task_text = \"\"\"\n",
    "The brute‐force scaleup of training datasets, learnable parameters and computation power, has become a prevalent strategy for developing more robust learning models. \n",
    "However, due to bottlenecks in data, computation, and trust, the sustainability of this strategy is a serious concern. \n",
    "In this paper, we attempt to address this issue in a parsimonious manner (i.e., achieving greater potential with simpler models). \n",
    "The key is to drive models using domain‐specific knowledge, such as symbols, logic, and formulas, instead of purely relying on scaleup. \n",
    "This approach allows us to build a framework that uses this knowledge as \"building blocks\" to achieve parsimony in model design, training, and interpretation. \n",
    "Empirical results show that our methods surpass those that typically follow the scaling law. We also demonstrate our framework in AI for science, specifically in the problem of drug‐drug interaction prediction. \n",
    "We hope our research can foster more diverse technical roadmaps in the era of foundation models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252eeb2",
   "metadata": {
    "height": 30
   },
   "source": [
    "Approach 1 - Basic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e3aaa64",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC:  Scaling up training datasets, parameters, and computation power is common for robust learning models, but sustainability is a concern. This paper proposes a parsimonious approach using domain-specific knowledge to build more efficient models.\n"
     ]
    }
   ],
   "source": [
    "basic_msg = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Summarize this text into 30 words: {task_text}\"\n",
    "    }\n",
    "]\n",
    "output_basic = run_and_save(basic_msg)\n",
    "print(\"BASIC: \", output_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d22107",
   "metadata": {
    "height": 30
   },
   "source": [
    "Approach 2: Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2506a956",
   "metadata": {
    "height": 368
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEWSHOT:  The text discusses the limitations of scaling up training datasets and computation power in developing learning models, proposing a parsimonious approach driven by domain-specific knowledge for more sustainable and effective results.\n"
     ]
    }
   ],
   "source": [
    "few_shot_msg = [\n",
    "    # Example 1\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Text: 'The ocean covers 70% of Earth.' \\n Q: Summarize this text.\\nA: The ocean covers most of the planet.\"\n",
    "    },\n",
    "    # Example 2\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Text: 'Bees pollinate many crops.'\\nQ: Summarize this text.\\nA: Bees help pollinate food plants.\"\n",
    "    },\n",
    "    # Now your real task\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "         \"content\": f\"Text: '''{task_text}'''\\nQ: Summarize this text in 30 words.\\nA:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "out_few = run_and_save(few_shot_msg)\n",
    "print(\"FEWSHOT: \", out_few)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f6233",
   "metadata": {
    "height": 30
   },
   "source": [
    "Appraoch 3: Chain-Of-Thought Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76d0feed",
   "metadata": {
    "height": 232
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT:  Brute-force scaling up of training data, parameters, and computation power is common but faces sustainability challenges. A parsimonious approach using domain-specific knowledge can lead to more effective learning models.\n"
     ]
    }
   ],
   "source": [
    "cot_msg = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert summarizer.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Think step by step, then summarize this text in 30 words:\\n\\n{task_text}\"\n",
    "    }\n",
    "]\n",
    "out_cot = run_and_save(cot_msg)\n",
    "print(\"COT: \", out_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebfc33",
   "metadata": {
    "height": 30
   },
   "source": [
    "Outputs\n",
    "\n",
    "1. BASIC:  Scaling up training datasets, parameters, and computation power is common for robust learning models, but sustainability is a concern. This paper proposes a parsimonious approach using domain-specific knowledge to build more efficient models.\n",
    "\n",
    "\n",
    "2. FEWSHOT:  The text discusses the limitations of scaling up training datasets and computation power in developing learning models, proposing a parsimonious approach driven by domain-specific knowledge for more sustainable and effective results.\n",
    "\n",
    "\n",
    "3. COT:  Brute-force scaling up of training data, parameters, and computation power is common but faces sustainability challenges. A parsimonious approach using domain-specific knowledge can lead to more effective learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
